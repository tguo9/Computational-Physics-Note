{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics\n",
    "\n",
    "## 1. Quick PCA review: The difference between \n",
    "\n",
    "### pca.fit() \n",
    "### pca.transform()\n",
    "### pca.fit_transform()\n",
    "\n",
    "## 2. Support Vector Machine (SVM)\n",
    "\n",
    "## 3. SVM Applied to Handwritten Digit Recognition\n",
    "\n",
    "## 4. The meaning of gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC: Support Vector Classification\n",
    "## In sklearn.svm.SVC() the default kernel is RBF\n",
    "## For kernel parameters, see\n",
    "## http://scikit-learn.org/stable/modules/svm.html#svm-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> float64 (1797, 64)\n",
      "<class 'numpy.ndarray'> float64 (1797, 8, 8)\n",
      "<class 'numpy.ndarray'> int64 (1797,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "''' Initial Imports'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# ***use seaborn plotting style defaults\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "#********************* KEY IMPORT OF THIS LECTURE********************************\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "# loading handwritten digits\n",
    "dig_data = load_digits()\n",
    "X = dig_data.data\n",
    "# y: the values of the digits, or \"ground truth\"\n",
    "y = dig_data.target\n",
    "dig_img = dig_data.images\n",
    "print(type(X), X.dtype, X.shape)\n",
    "print(type(dig_img), dig_img.dtype, dig_img.shape)\n",
    "print(type(y), y.dtype, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's explore the documentation for SVC \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "## and the attributes:\n",
    "\n",
    "- ### support\\_ : array-like, shape = [n\\_SV]\n",
    "Indices of support vectors.\n",
    "\n",
    "\n",
    "- ### support\\_vectors\\_ : array-like, shape = [n_SV, n_features]\n",
    "Support vectors.\n",
    "\n",
    "- ### n\\_support\\_ : array-like, dtype=int32, shape = [n_class]\n",
    "Number of support vectors for each class.\n",
    "\n",
    "- ### dual\\_coef\\_ : array, shape = [n_class-1, n_SV]\n",
    "dual\\_coef\\_ : array, shape = [n_class-1, n_SV]\n",
    "Coefficients of the support vector in the decision function. For multiclass, coefficient for all 1-vs-1 classifiers. The layout of the coefficients in the multiclass case is somewhat non-trivial. See the section about multi-class classification in the SVM section of the User Guide for details.\n",
    "\n",
    "    --> These can be converted to the Langrange multipliers (i.e., the weights; except here, with labels mixed in)\n",
    "    \n",
    "- ### intercept_ : array, shape = [n_class * (n_class-1) / 2]\n",
    "Constants in decision function.  \n",
    "\n",
    "    --> That is, the b! (I have thought much about the shape yet -- need to do that.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying SVM Handwritten Digit Recogntion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters to specify:\n",
    "\n",
    "- ### kernel: The default is 'rbf'.  For other kernels, see\n",
    "\n",
    "    ### http://scikit-learn.org/stable/modules/svm.html#kernel-functions\n",
    "\n",
    "- ###  Setting C: C is 1 by default and itâ€™s a reasonable default choice. If you have a lot of noisy observations you should decrease it. It corresponds to \"regularize\" the estimation: The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth (ignores noise), while a high C aims at classifying all training examples correctly (but maybe giving noise too much weight). [We will addres the issue of overfitting in Computational Physics II.]\n",
    "\n",
    "- ### If data for classification are unbalanced (e.g. many positive and few negative), set class_weight='balanced' and/or try different penalty parameters C.\n",
    "\n",
    "- ### (Particular to RBF) gamma: it defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> float64 (1797, 64)\n",
      "<class 'numpy.ndarray'> int64 (1797,)\n",
      "Xtrain.shape, ytrain.shape (1796, 64) (1796,)\n",
      "Xtest.shape, ytest.shape (1, 64) ()\n",
      "(1796, 10)\n",
      "(1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPQAAAD3CAYAAAAqu3lQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAA+ZJREFUeJzt3VFNY10YRuHDH+5BQh1QCyhoJYAD\nJBQH4KQ4QEJxUAk4OGNg/oTMkK+TxfMYePc5ycq+3Ffruq4LkPDfpQ8AfB9BQ4igIUTQECJoCLm+\n9AH4vff397Gt/X4/trXdbse2Jv/hv8INDSGChhBBQ4igIUTQECJoCBE0hAgaQgQNIYKGEEFDiKAh\nRNAQImgIETSECBpCBA0hgoYQQUOIoCFE0BAiaAgRNIQIGkIEDSGChhBP4XzR6XQa3bu/vx/burm5\nGds6n89jWz+RGxpCBA0hgoYQQUOIoCFE0BAiaAgRNIQIGkIEDSGChhBBQ4igIUTQECJoCBE0hAga\nQgQNIYKGEEFDiKAhRNAQImgIETSECBpCBA0hnsL5ouPxOLp3d3c3trXf78e2np+fx7Z+Ijc0hAga\nQgQNIYKGEEFDiKAhRNAQImgIETSECBpCBA0hgoYQQUOIoCFE0BAiaAgRNIQIGkIEDSGChhBBQ4ig\nIUTQECJoCBE0hAgaQgQNId62+qKnp6fRvc1mM7Y1+W273W5s6ydyQ0OIoCFE0BAiaAgRNIQIGkIE\nDSGChhBBQ4igIUTQECJoCBE0hAgaQgQNIYKGEEFDiKAhRNAQImgIETSECBpCBA0hgoYQQUOIoCHk\nal3X9dKH+FOfn59jWy8vL2Nby7Isx+NxbOt8Pie3bm9vx7b+FW5oCBE0hAgaQgQNIYKGEEFDiKAh\nRNAQImgIETSECBpCBA0hgoYQQUOIoCFE0BAiaAgRNIQIGkIEDSGChhBBQ4igIUTQECJoCBE0hFxf\n+gB/43A4jG29vr6ObU2bfHbnJz5PM8kNDSGChhBBQ4igIUTQECJoCBE0hAgaQgQNIYKGEEFDiKAh\nRNAQImgIETSECBpCBA0hgoYQQUOIoCFE0BAiaAgRNIQIGkIEDSGChpCrdV3XSx/iT51Op7Gth4eH\nsa1lWZaPj4/RvSm73W5s6/HxcWxrWWa/7f+4oSFE0BAiaAgRNIQIGkIEDSGChhBBQ4igIUTQECJo\nCBE0hAgaQgQNIYKGEEFDiKAhRNAQImgIETSECBpCBA0hgoYQQUOIoCFE0BAiaAi5vvQB/sZ2ux3b\nmnxHa3rvcDiMbb29vY1tbTabsa1l8bYV8M0EDSGChhBBQ4igIUTQECJoCBE0hAgaQgQNIYKGEEFD\niKAhRNAQImgIETSECBpCBA0hgoYQQUOIoCFE0BAiaAgRNIQIGkIEDSFX67qulz4E8D3c0BAiaAgR\nNIQIGkIEDSGChhBBQ4igIUTQECJoCBE0hAgaQgQNIYKGEEFDiKAhRNAQImgIETSECBpCBA0hgoaQ\nX8FFS6cEyXCJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0a162208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' \n",
    "Now, we get the first \"2\" right!\n",
    "\n",
    "The first \"5\" is still trouble, \n",
    "but even human can't necessarily tell that \"5\"!  \n",
    "\n",
    "'''\n",
    "dig_data = load_digits()\n",
    "X = dig_data.data\n",
    "y = dig_data.target\n",
    "# This is basically each array in X\n",
    "# getting reshaped into (8, 8).\n",
    "dig_img = dig_data.images\n",
    "\n",
    "print(type(X), X.dtype, X.shape)\n",
    "print(type(y), y.dtype, y.shape)\n",
    "\n",
    "select_idx = 2\n",
    "# select_idx = 5\n",
    "\n",
    "# ********************************Separating training data from testing data****************\n",
    "Xtrain = np.delete(X, select_idx, axis = 0)\n",
    "ytrain = np.delete(y, select_idx)\n",
    "\n",
    "# if you don't do .reshape(1, -1), you get a warning.\n",
    "# B/c the data argument for classifier has to be an array,\n",
    "# even if it's a one-element array.\n",
    "Xtest = X[select_idx].reshape(1, -1)\n",
    "test_img = dig_img[select_idx]\n",
    "ytest = y[select_idx]\n",
    "\n",
    "print('Xtrain.shape, ytrain.shape', Xtrain.shape, ytrain.shape)\n",
    "print('Xtest.shape, ytest.shape', Xtest.shape, ytest.shape)\n",
    "\n",
    "plt.figure(figsize = (4, 4))\n",
    "plt.imshow(test_img, cmap = 'binary')\n",
    "plt.grid('off')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "# ************************************* The PCA Section ********************************\n",
    "n_comp = 10\n",
    "\n",
    "pca = PCA(n_comp)  \n",
    "\n",
    "# finding pca axes\n",
    "pca.fit(Xtrain)\n",
    "# projecting training data onto pca axes\n",
    "Xtrain_proj = pca.transform(Xtrain)\n",
    "# projecting test data onto pca axes\n",
    "Xtest_proj = pca.transform(Xtest)\n",
    "\n",
    "print(Xtrain_proj.shape)\n",
    "print(Xtest_proj.shape)\n",
    "\n",
    "\n",
    "# ************************************* The SVM Section ********************************\n",
    "\n",
    "# instantiating an SVM classifier\n",
    "clf = svm.SVC(gamma=0.001, C=100.)\n",
    "\n",
    "# apply SVM to training data and draw boundaries.\n",
    "clf.fit(Xtrain_proj, ytrain)\n",
    "# Use SVM-determined boundaries to make\n",
    "# a prediction for the test data point.\n",
    "clf.predict(Xtest_proj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout:\n",
    "## 1. Turn the above into a function\n",
    "## classify_dig_svm(X, y, dig_img, select_idx, n_comp = 10, plot_test_img = False)\n",
    "## where\n",
    "- ### X: data\n",
    "- ### y: targets (labels, or \"ground truth\")\n",
    "- ### select_idx: the index of the test data point\n",
    "- ### dig_img: 2D arrays of the digit image that corresponds to select_idx\n",
    "- ### plot_test_img: plot the above image, if True.\n",
    "- ### n_comp: how many PCA components to use\n",
    "- ### returns the prediction\n",
    "\n",
    "## 2. Test this function on select_idx = 0, 1, and 2.  One at a time.\n",
    "\n",
    "## 3. Write a main program that does the \"leave-one-out\" test first for 50 images, just as we did in last class, 10 components; then move up to 20 components.  Compute the success rate in each case.\n",
    "\n",
    "## 4. Do the \"leave-one-out\" test for 500 images for 10 and then 20 components.  Compute the success rate for each case (takes about 1 min).\n",
    "\n",
    "## The success rates for all 1797 images with 20 (or 30) components are similar to 500 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def classify_dig_svm(X, y, dig_img, select_idx, n_comp = 10, plot_test_img = False):\n",
    "\n",
    "#     print(type(X), X.dtype, X.shape)\n",
    "#     print(type(y), y.dtype, y.shape)\n",
    "\n",
    "    # ********************************Separating training data from testing data****************\n",
    "    Xtrain = np.delete(X, select_idx, axis = 0)\n",
    "    ytrain = np.delete(y, select_idx)\n",
    "\n",
    "    # if you don't do .reshape(1, -1), you get a warning.\n",
    "    # B/c the data argument for classifier has to be an array,\n",
    "    # even if it's a one-element array.\n",
    "    Xtest = X[select_idx].reshape(1, -1)\n",
    "    test_img = dig_img[select_idx]\n",
    "    ytest = y[select_idx]\n",
    "\n",
    "#     print('Xtrain.shape, ytrain.shape', Xtrain.shape, ytrain.shape)\n",
    "#     print('Xtest.shape, ytest.shape', Xtest.shape, ytest.shape)\n",
    "    \n",
    "    if (plot_test_img == True):\n",
    "        plt.figure(figsize = (4, 4))\n",
    "        plt.imshow(test_img, cmap = 'binary')\n",
    "        plt.grid('off')\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "    # ************************************* The PCA Section ********************************\n",
    "\n",
    "    pca = PCA(n_comp)  \n",
    "\n",
    "    # finding pca axes\n",
    "    pca.fit(Xtrain)\n",
    "    # projecting training data onto pca axes\n",
    "    Xtrain_proj = pca.transform(Xtrain)\n",
    "    # projecting test data onto pca axes\n",
    "    Xtest_proj = pca.transform(Xtest)\n",
    "\n",
    "#     print(Xtrain_proj.shape)\n",
    "#     print(Xtest_proj.shape)\n",
    "\n",
    "\n",
    "    # ************************************* The SVM Section ********************************\n",
    "\n",
    "    # instantiating an SVM classifier\n",
    "    clf = svm.SVC(gamma=0.001, C=100.)\n",
    "\n",
    "    # apply SVM to training data and draw boundaries.\n",
    "    clf.fit(Xtrain_proj, ytrain)\n",
    "    # Use SVM-determined boundaries to make\n",
    "    # a prediction for the test data point.\n",
    "\n",
    "    return clf.predict(Xtest_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "dig_data = load_digits()\n",
    "X = dig_data.data\n",
    "y = dig_data.target\n",
    "dig_img = dig_data.images\n",
    "digits = dig_data.target\n",
    "\n",
    "num = 50\n",
    "counter = 1\n",
    "\n",
    "for i in range(num):\n",
    "    prediction = classify_dig_svm(X, y, dig_img, i, n_comp = 10)\n",
    "    if(prediction == digits[i]):\n",
    "        counter += 1\n",
    "rate = counter/num\n",
    "print(rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of week 15-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
